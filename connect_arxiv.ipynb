{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f70119e-53c1-4bc2-bcc2-c9123008933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Collecting feedparser~=6.0.10\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests~=2.32.0 in /home/users/sv226/.local/lib/python3.10/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in /usr/lib/python3/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests~=2.32.0->arxiv) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests~=2.32.0->arxiv) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests~=2.32.0->arxiv) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/users/sv226/.local/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Installing collected packages: feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f68bb02-3aa2-4ec5-bcd5-dd0a2782999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1150988/1236020001.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention\n",
      "[arxiv.Result.Author('Shang Yang'), arxiv.Result.Author('Junxian Guo'), arxiv.Result.Author('Haotian Tang'), arxiv.Result.Author('Qinghao Hu'), arxiv.Result.Author('Guangxuan Xiao'), arxiv.Result.Author('Jiaming Tang'), arxiv.Result.Author('Yujun Lin'), arxiv.Result.Author('Zhijian Liu'), arxiv.Result.Author('Yao Lu'), arxiv.Result.Author('Song Han')]\n",
      "Large language models (LLMs) have shown remarkable potential in processing\n",
      "long sequences, yet efficiently serving these long-context models remains\n",
      "challenging due to the quadratic computational complexity of attention in the\n",
      "prefilling stage and the large memory footprint of the KV cache in the decoding\n",
      "stage. To address these issues, we introduce LServe, an efficient system that\n",
      "accelerates long-sequence LLM serving via hybrid sparse attention. This method\n",
      "unifies different hardware-friendly, structured sparsity patterns for both\n",
      "prefilling and decoding attention into a single framework, where computations\n",
      "on less important tokens are skipped block-wise. LServe demonstrates the\n",
      "compatibility of static and dynamic sparsity in long-context LLM attention.\n",
      "This design enables multiplicative speedups by combining these optimizations.\n",
      "Specifically, we convert half of the attention heads to nearly free streaming\n",
      "heads in both the prefilling and decoding stages. Additionally, we find that\n",
      "only a constant number of KV pages is required to preserve long-context\n",
      "capabilities, irrespective of context length. We then design a hierarchical KV\n",
      "page selection policy that dynamically prunes KV pages based on query-centric\n",
      "similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\n",
      "decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\n",
      "released at https://github.com/mit-han-lab/omniserve.\n",
      "http://arxiv.org/pdf/2502.14866v1\n",
      "Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts\n",
      "[arxiv.Result.Author('Sara Ghaboura'), arxiv.Result.Author('Ketan More'), arxiv.Result.Author('Ritesh Thawkar'), arxiv.Result.Author('Wafa Alghallabi'), arxiv.Result.Author('Omkar Thawakar'), arxiv.Result.Author('Fahad Shahbaz Khan'), arxiv.Result.Author('Hisham Cholakkal'), arxiv.Result.Author('Salman Khan'), arxiv.Result.Author('Rao Muhammad Anwer')]\n",
      "Understanding historical and cultural artifacts demands human expertise and\n",
      "advanced computational techniques, yet the process remains complex and\n",
      "time-intensive. While large multimodal models offer promising support, their\n",
      "evaluation and improvement require a standardized benchmark. To address this,\n",
      "we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning\n",
      "266 distinct cultures across 10 major historical regions. Designed for\n",
      "AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological\n",
      "discoveries, TimeTravel provides a structured dataset and robust evaluation\n",
      "framework to assess AI models' capabilities in classification, interpretation,\n",
      "and historical comprehension. By integrating AI with historical research,\n",
      "TimeTravel fosters AI-powered tools for historians, archaeologists,\n",
      "researchers, and cultural tourists to extract valuable insights while ensuring\n",
      "technology contributes meaningfully to historical discovery and cultural\n",
      "heritage preservation. We evaluate contemporary AI models on TimeTravel,\n",
      "highlighting their strengths and identifying areas for improvement. Our goal is\n",
      "to establish AI as a reliable partner in preserving cultural heritage, ensuring\n",
      "that technological advancements contribute meaningfully to historical\n",
      "discovery. Our code is available at:\n",
      "\\url{https://github.com/mbzuai-oryx/TimeTravel}.\n",
      "http://arxiv.org/pdf/2502.14865v1\n",
      "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling\n",
      "[arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Tengyu Pan'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Yudi Zhang'), arxiv.Result.Author('Ao Sun'), arxiv.Result.Author('Yuxiang Huang'), arxiv.Result.Author('Kaihuo Zhang'), arxiv.Result.Author('Weilun Zhao'), arxiv.Result.Author('Yuxuan Li'), arxiv.Result.Author('Jianyong Wang'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun')]\n",
      "Speculative sampling has emerged as an important technique for accelerating\n",
      "the auto-regressive generation process of large language models (LLMs) by\n",
      "utilizing a draft-then-verify mechanism to produce multiple tokens per forward\n",
      "pass. While state-of-the-art speculative sampling methods use only a single\n",
      "layer and a language modeling (LM) head as the draft model to achieve\n",
      "impressive layer compression, their efficiency gains are substantially reduced\n",
      "for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\n",
      "To address this, we present FR-Spec, a frequency-ranked speculative sampling\n",
      "framework that optimizes draft candidate selection through vocabulary space\n",
      "compression. By constraining the draft search to a frequency-prioritized token\n",
      "subset, our method reduces LM Head computation overhead by 75% while ensuring\n",
      "the equivalence of the final output distribution. Experiments across multiple\n",
      "datasets demonstrate an average of 1.12$\\times$ speedup over the\n",
      "state-of-the-art speculative sampling method EAGLE-2.\n",
      "http://arxiv.org/pdf/2502.14856v1\n",
      "Prompt-to-Leaderboard\n",
      "[arxiv.Result.Author('Evan Frick'), arxiv.Result.Author('Connor Chen'), arxiv.Result.Author('Joseph Tennyson'), arxiv.Result.Author('Tianle Li'), arxiv.Result.Author('Wei-Lin Chiang'), arxiv.Result.Author('Anastasios N. Angelopoulos'), arxiv.Result.Author('Ion Stoica')]\n",
      "Large language model (LLM) evaluations typically rely on aggregated metrics\n",
      "like accuracy or human preference, averaging across users and prompts. This\n",
      "averaging obscures user- and prompt-specific variations in model performance.\n",
      "To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\n",
      "leaderboards specific to a prompt. The core idea is to train an LLM taking\n",
      "natural language prompts as input to output a vector of Bradley-Terry\n",
      "coefficients which are then used to predict the human preference vote. The\n",
      "resulting prompt-dependent leaderboards allow for unsupervised task-specific\n",
      "evaluation, optimal routing of queries to models, personalization, and\n",
      "automated evaluation of model strengths and weaknesses. Data from Chatbot Arena\n",
      "suggest that P2L better captures the nuanced landscape of language model\n",
      "performance than the averaged leaderboard. Furthermore, our findings suggest\n",
      "that P2L's ability to produce prompt-specific evaluations follows a power law\n",
      "scaling similar to that observed in LLMs themselves. In January 2025, the\n",
      "router we trained based on this methodology achieved the \\#1 spot in the\n",
      "Chatbot Arena leaderboard. Our code is available at this GitHub link:\n",
      "https://github.com/lmarena/p2l.\n",
      "http://arxiv.org/pdf/2502.14855v1\n",
      "Dynamic Concepts Personalization from Single Videos\n",
      "[arxiv.Result.Author('Rameen Abdal'), arxiv.Result.Author('Or Patashnik'), arxiv.Result.Author('Ivan Skorokhodov'), arxiv.Result.Author('Willi Menapace'), arxiv.Result.Author('Aliaksandr Siarohin'), arxiv.Result.Author('Sergey Tulyakov'), arxiv.Result.Author('Daniel Cohen-Or'), arxiv.Result.Author('Kfir Aberman')]\n",
      "Personalizing generative text-to-image models has seen remarkable progress,\n",
      "but extending this personalization to text-to-video models presents unique\n",
      "challenges. Unlike static concepts, personalizing text-to-video models has the\n",
      "potential to capture dynamic concepts, i.e., entities defined not only by their\n",
      "appearance but also by their motion. In this paper, we introduce\n",
      "Set-and-Sequence, a novel framework for personalizing Diffusion Transformers\n",
      "(DiTs)-based generative video models with dynamic concepts. Our approach\n",
      "imposes a spatio-temporal weight space within an architecture that does not\n",
      "explicitly separate spatial and temporal features. This is achieved in two key\n",
      "stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\n",
      "unordered set of frames from the video to learn an identity LoRA basis that\n",
      "represents the appearance, free from temporal interference. In the second\n",
      "stage, with the identity LoRAs frozen, we augment their coefficients with\n",
      "Motion Residuals and fine-tune them on the full video sequence, capturing\n",
      "motion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\n",
      "weight space that effectively embeds dynamic concepts into the video model's\n",
      "output domain, enabling unprecedented editability and compositionality while\n",
      "setting a new benchmark for personalizing dynamic concepts.\n",
      "http://arxiv.org/pdf/2502.14844v1\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query=\"cat:cs.LG\",\n",
    "  max_results=5,\n",
    "  sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "for result in search.results():\n",
    "    print(result.title)\n",
    "    print(result.authors)\n",
    "    print(result.summary)\n",
    "    print(result.pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b0ad1-f082-4080-8cd2-c7ea18686b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
