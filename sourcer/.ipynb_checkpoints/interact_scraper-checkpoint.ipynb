{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reddit_scraper as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "# Initialize the Reddit instance\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='KLBeMAiqwWYtnZV9k0LI4Q',\n",
    "    client_secret='ZAZbrGYK8hti4ISSZOZ3INq9Y3bgCg',\n",
    "    user_agent='arxiv_scraper'\n",
    ")\n",
    "\n",
    "# URL of the Reddit post\n",
    "url = 'https://www.reddit.com/r/MachineLearning/comments/1it790b/r_diffusion_is_the_solution_for_efficient_and/'\n",
    "\n",
    "# Fetch the submission using the URL\n",
    "submission = reddit.submission(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3189860/1659709744.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselftext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'submission' is not defined"
     ]
    }
   ],
   "source": [
    "submission.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:\\n\\n\"Given the breadth and depth of GPT-4\\'s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"\\n\\nWhat are everyone\\'s thoughts?',\n",
       " \"Researchers from Google and DeepMind have developed and evaluated an LLM fine-tuned specifically for clinical diagnostic reasoning. In a new study, they rigorously tested the LLM's aptitude for generating differential diagnoses and aiding physicians.\\n\\nThey assessed the LLM on 302 real-world case reports from the New England Journal of Medicine. These case reports are known to be highly complex diagnostic challenges.\\n\\nThe LLM produced differential diagnosis lists that included the final confirmed diagnosis in the top 10 possibilities in 177 out of 302 cases, a top-10 accuracy of 59%. **This significantly exceeded the performance of experienced physicians, who had a top-10 accuracy of just 34% on the same cases when unassisted.**\\n\\nAccording to assessments from senior specialists, the LLM's differential diagnoses were also rated to be **substantially more appropriate and comprehensive** than those produced by physicians, when evaluated across all 302 case reports.\\n\\nThis research demonstrates the potential for LLMs to enhance physicians' clinical reasoning abilities for complex cases. However, the authors emphasize that further rigorous real-world testing is essential before clinical deployment. Issues around model safety, fairness, and robustness must also be addressed.\\n\\n[**Full summary**](https://aimodels.substack.com/p/googles-new-llm-doctor-is-right-way). [**Paper**](https://arxiv.org/abs/2401.05654).\",\n",
       " 'In a recent paper, researchers have discovered that LLMs show enhanced performance when provided with prompts infused with emotional context, which they call \"EmotionPrompts.\"\\n\\nThese prompts incorporate sentiments of urgency or importance, such as \"It\\'s crucial that I get this right for my thesis defense,\" as opposed to neutral prompts like \"Please provide feedback.\"\\n\\nThe study\\'s empirical evidence suggests substantial gains. This indicates a **significant sensitivity of LLMs to the implied emotional stakes** in a prompt:\\n\\n* Deterministic tasks saw an 8% performance boost\\n* Generative tasks experienced a 115% improvement when benchmarked using BIG-Bench.\\n* Human evaluators further validated these findings, observing a 10.9% increase in the perceived quality of responses when EmotionPrompts were used.\\n\\nThis enhancement is attributed to the models\\' capacity to detect and prioritize the heightened language patterns that imply a need for precision and care in the response.\\n\\nThe research delineates the potential of EmotionPrompts to refine the effectiveness of AI in applications where understanding the user\\'s intent and urgency is paramount, even though the AI does not genuinely comprehend or feel emotions.\\n\\n**TLDR: Research shows LLMs deliver better results when prompts signal emotional urgency. This insight can be leveraged to improve AI applications by integrating EmotionPrompts into the design of user interactions.**\\n\\n[Full summary is here](https://aimodels.substack.com/p/telling-gpt-4-youre-scared-or-under). Paper [here](https://arxiv.org/pdf/2307.11760.pdf).',\n",
       " '>We present a fundamental\\xa0discovery\\xa0that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (often\\xa0>100,000\\xa0examples), we demonstrate a striking phenomenon: complex mathematical reasoning abilities can be effectively elicited with\\xa0surprisingly\\xa0few examples. This finding challenges not only the assumption of massive data requirements but also the common belief that supervised fine-tuning primarily leads to memorization rather than generalization. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance and efficiency in mathematical reasoning. With merely\\xa0817\\xa0curated training samples, LIMO achieves\\xa057.1%\\xa0accuracy on the highly challenging AIME benchmark and\\xa094.8%\\xa0on MATH, improving the performance of previous strong SFT-based models from 6.5% to 57.1% on AIME and from 59.2% to 94.8% on MATH, while only using 1% of the training data required by previous approaches. Most remarkably, LIMO demonstrates exceptional out-of-distribution generalization, achieving\\xa040.5%\\xa0absolute improvement across\\xa010\\xa0diverse benchmarks, outperforming models trained on 100x more data, directly challenging the prevailing notion that SFT inherently leads to memorization rather than generalization. Synthesizing these pioneering results, we propose the\\xa0Less-Is-More Reasoning Hypothesis (LIMO Hypothesis):\\xa0In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the\\xa0elicitation threshold\\xa0for complex reasoning is not inherently bounded by the complexity of the target reasoning task, but fundamentally determined by two key factors: (1) the completeness of the model’s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples, which serve as “cognitive templates” that show the model how to effectively utilize its existing knowledge base to solve complex reasoning tasks.\\n\\nArxiv link: [\\\\[2502.03387\\\\] LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387)',\n",
       " \"Instead of using gradient descent to minimize a single loss, we propose to use *Jacobian descent* to minimize multiple losses simultaneously. Basically, this algorithm updates the parameters of the model by reducing the Jacobian of the (vector-valued) objective function into an update vector.\\n\\nTo make it accessible to everyone, we have developed *TorchJD*: a library extending autograd to support Jacobian descent. After a simple `pip install torchjd`, transforming a PyTorch-based training function is very easy. With the recent release v0.2.0, TorchJD finally supports multi-task learning!\\n\\nGithub: [https://github.com/TorchJD/torchjd](https://github.com/TorchJD/torchjd)  \\nDocumentation: [https://torchjd.org](https://torchjd.org)  \\nPaper: [https://arxiv.org/pdf/2406.16232](https://arxiv.org/pdf/2406.16232)\\n\\nWe would love to hear some feedback from the community. If you want to support us, a star on the repo would be grealy appreciated! We're also open to discussion and criticism.\",\n",
       " 'Paper - https://arxiv.org/abs/2305.07759',\n",
       " \"In my latest research ([here's the paper](https://arxiv.org/abs/2502.03787)), I prove accelerated convergence of iterative reasoning frameworks like chain-of-thought, my last paper[ contextual feedback loops](https://arxiv.org/abs/2412.17737). I also prove that feedforward models require a network with an exponentially greater depth than recurrent structures to achieve the same level of accuracy. These are all under mild assumptions.\\n\\nIf you are into ML theory, it's an interesting read (in my biased opinion).  Again, here are the main points of the paper:\\n\\n* **Accelerated Convergence:**\\n   * **What It Means:** The paper proves that when there is no persistent noise, the iterative reasoning framework converges to its target (or fixed point) at an optimal rate that scales as O(1/t\\\\^2). Here, t represents the algorithm's number of iterations or update steps. Essentially, as you run more iterations, the error decreases quadratically fast.\\n   * **In-Depth:** Even when the update process is subject to adaptive, state-dependent perturbations (small, possibly changing errors at each step), the method maintains this rapid convergence rate under the proper smoothness and contractivity assumptions. With each iteration, the process makes significant progress toward the final solution, making it highly efficient in ideal (noise-free) scenarios.\\n* **Feedback/Recurrent Necessity:**\\n   * **What It Means:** The analysis demonstrates that feedback (or iterative/recurrent) architectures—where the output of one step is fed back into the next—are crucial for efficiently approximating fixed-point functions. A fixed-point function is one where applying the function repeatedly eventually leads to a stable value (the fixed point).\\n   * **In-Depth:** The paper shows that using such iterative methods, one can achieve the desired approximation with a number of iterations that scales polynomially (like O(1/\\\\\\\\sqrt{ϵ}) for a given error ϵ). In contrast, feedforward models, which do not loop back on their own outputs but instead compute the answer in a single forward pass through layers, would require a network with an exponentially greater depth to match the same level of accuracy. This underlines the importance of designing systems with feedback loops to efficiently handle complex reasoning tasks.\",\n",
       " '**Competitive Programming with Large Reasoning Models**\\n\\n*OpenAI*\\n\\nWe show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.\\n\\nhttps://arxiv.org/abs/2502.06807',\n",
       " 'Meta AI (FAIR) latest paper integrates system-1 and system-2 thinking into reasoning models.\\n\\nBasically, it introduces the term \"Dualformer\" which integrates both system-1 (fast-thinking) and system-2 (slow-thinking) into the transformer to improve its reasoning capability. The high level idea is to train the model with \"randomized trace\", which randomly drop parts of the reasoning tokens. This approach improves model\\'s inference speed, accuracy, and diversity. It also enables model to perform system-1 and system-2 thinking in a controllable fashion. \\n\\nThe paper\\'s link here:\\n\\n[https://arxiv.org/html/2410.09918v1](https://arxiv.org/html/2410.09918v1)',\n",
       " 'https://arxiv.org/abs/2410.01201\\n\\nThe authors (including Y. Bengio) propose simplified versions of LSTM and GRU that allow parallel training, and show strong results on some benchmarks.',\n",
       " 'TL;DR and paper link are at the bottom of the post.\\n\\nI\\'m an undergrad who just wrote my first paper completely solo. Crazy experience with so many highs and lows, but I learned a lot from it. I think the results are important and I want people to see them, so I\\'ll try to walk through the paper here as best as I can.\\n\\nGiven the nature of Reddit posts, I\\'ll focus a bit less on the methods and more on the results. I won\\'t cite stuff here either, but obviously you can find citations in the paper.\\n\\nFirst I\\'ll give a small bit of historical context to what I\\'m doing, then walk through what I did and what came of it.\\n\\nEnjoy the read.\\n\\n# The general intelligence factor in humans\\n\\nIn the early 1900s, Charles Spearman observed that children\\'s performance across diverse school subjects was positively correlated (pictured below). He proposed the concept of a \"general intelligence factor,\" or *g*, to account for this correlation. This is why factor analysis was invented, it was invented by Spearman to quantify *g*.\\n\\n&#x200B;\\n\\n[The OG correlation matrix of school subjects](https://preview.redd.it/ohzhx16h6sub1.png?width=456&format=png&auto=webp&s=d9e0dd8e7b33571618cc2aa3399edabfbd873c12)\\n\\nA century of research later, *g* has proven to be a robust and reliable construct. The positive correlations between various mental abilities, known as the positive manifold, have become one of the most replicated findings in differential psychology. The *g* factor typically accounts for over 40% of the variance in cognitive ability tests and serves as a strong predictor for various life outcomes.\\n\\nWhile Spearman\\'s original two-factor model suggested that intelligence comprises a general factor *g* and specific factors *s* unique to each test, contemporary research has refined this view. Current consensus holds that *g* sits atop a hierarchical model akin to the one shown below, underpinned by several first-order factors.\\n\\nhttps://preview.redd.it/9cheo29n6sub1.png?width=973&format=png&auto=webp&s=b2eadc486f9727933b24d9f808c3f7effc1b5fd0\\n\\n# The general intelligence factor in non-human animals\\n\\nThe notion of general intelligence in non-human animals has been a subject of interest since the 1930, shortly after Spearman\\'s concept gained traction. Empirical evidence suggests that *g* is not exclusive to humans. For instance, in rodents like mice, a *g* factor accounts for approximately 35% of the variance in cognitive performance. In a comprehensive meta-analysis covering non-human primates, a single factor explained 47% of the variance across 62 species, indicating a *g* factor similar to that in humans. Even in some bird species, such as bowerbirds, *g* explains over 44% of the variance in cognitive abilities.\\n\\nHowever, it\\'s worth noting that *g* may not be universal across all species. For example, evidence suggests that fish may not possess a *g* factor. Despite limitations like low sample size or limited task diversity in research on non-human animals, these findings indicate that *g* is not unique to humans and can sometimes be observed in various non-human species.\\n\\n# Does g exist in language models?\\n\\nI suspected *g* might exist in language models and prove itself to be both a powerful explanatory variable and an invaluable tool for measuring LLM ability.\\n\\nTo test for it\\'s existence, I analyzed 1,232 models from the Open LLM Leaderboard and 88 models from the General Language Understanding Evaluation (GLUE) Leaderboard. A variety of cognitive subtests were used to assess the models, including ARC Challenge, Hellaswag,  TruthfulQA, MMLU subtests seen in the images below. Factor analysis techniques, specifically principal axis factoring, were employed to extract *g* from the performance data.\\n\\n&#x200B;\\n\\nhttps://preview.redd.it/oz2yb78x6sub1.png?width=1103&format=png&auto=webp&s=92a853321e015fe17ba89637e0c3c3bf9d71cd14\\n\\n&#x200B;\\n\\nhttps://preview.redd.it/9q0an7k07sub1.png?width=1139&format=png&auto=webp&s=e18f216e1b880117a819ca17cda038d66889dcf9\\n\\nAs can be seen, correlations are uniformly positive (and extremely high) between all subtests, showing the existence of a \"positive manifold\". The average correlation in the matrices is .84, exactly the same for both datasets.\\n\\nThere was agreement for all statistical tests across both datasets that a single factor should be extracted (with only a single exception which was dismissed, as discussed in detail in the paper).\\n\\nAfter factor analysis was performed, *g* loadings for subtests were obtained. Loosely speaking, the *g* loading is a correlation between *g* and the specific subtest.\\n\\n&#x200B;\\n\\nhttps://preview.redd.it/m9xuj5c97sub1.png?width=435&format=png&auto=webp&s=8aad5fdaa2dbfa015fb317004c4d6af1dfc163bd\\n\\nFor the sake of brevity I won\\'t post the subtest loading table for GLUE, but that\\'s in the original paper as well. In there, loadings are .78 to .97 approximately.\\n\\nNow here is an example of how we can rank models according to their general ability:\\n\\n&#x200B;\\n\\nhttps://preview.redd.it/hrrbvwkg7sub1.png?width=498&format=png&auto=webp&s=9afa927a7f0674a8946c6b6f5beaae9d1bb63099\\n\\nIn conclusion, both datasets showed an existence of *g* in language models. We now have a new unified method of ranking models based on how generally capable they are across tasks.\\n\\n# How \"strong\" is g in language models?\\n\\nAbout twice as strong as in humans and some animals.\\n\\nThe *g* factor in language models explains 85% of the variance on all tasks, in contrast to roughly 40% for humans and some animals. The number 85% is exactly replicated in both datasets.\\n\\nThe subtask *g* loading averages about .92, significantly higher than about .6 for humans.\\n\\n# How reliable is g in language models?\\n\\nAfter confirming that *g* is reliable across populations (i.e. it exists in both datasets), the study also included reliability analyses to assess the stability of *g* across test batteries and methods of extraction. In short, I wanted to see if we are actually measuring the same thing when we extract *g* from the same language models tested on 2 completely different test batteries.\\n\\nI\\'ll spare you the details on this one, but the correlation between *g* extracted from disjoint test batteries is basically 1. Same goes for different methods of extraction of *g*, like using PCA instead of FA. The *g* factor is therefore unique and highly reliable.\\n\\n# Correlation between model size and g\\n\\nFinally, the relationship between model size and *g* was explored. In short, the correlation was found to be r = .48 (p < .0001; 95% CI \\\\[.44, .52\\\\]). So, there exists a moderate/strong positive relationship between model size and *g*.\\n\\n# Implications & Future Research\\n\\nThe identification of *g* in language models firstly allows us to measure what we actually want to measure (and compare) in language models, that is general ability. It allows the whole field to have a unified metric that can be used whenever we care more about general ability than some specific ability (like virology knowledge), which is almost always the case.\\n\\nAnother benefit of using *g* as the primary measure of ability in language models is that it prevents researchers fiddling with the administered test(s) until you find the specific test which seems to show that your model is better than the rest. It standardizes ability measurements in LLMs.\\n\\nPlus, even if your improvement in a specific ability is real and not HARKed / p-hacked to death, it may still be just that, an improvement in specific abilities that don\\'t affect general intelligence at all. This is obviously important to know when an improvement is discussed, and *g* is the measure that can tell us which is it. As an example of specific non-*g* improvements in humans, look up \"Flynn effect\".\\n\\nI\\'d argue there\\'s a big resource efficiency gain too, because now you can evaluate your model on a few carefully chosen *g*\\\\-loaded subtests, derive *g* and infer the model\\'s performance on all other tasks instead of testing your model on 200 tests each with 50+ items (like BigBench does, for example).\\n\\nApart from that, this method also allows for an objective ranking of various tests based on their *g* loading, which in turn provides a standardized measure of test relevance for specific populations of language models.\\n\\nAs for future research, there\\'s tons of things to do. I\\'m personally interested in confirming the factor structure of general intelligence in LLMs or seeing impact of fine-tuning and RLHF on *g*. One can also examine which variables other than model size explain variance in *g* or how general ability and social bias correlate. I\\'d have loved to do these things, and it wouldn\\'t even be hard, but I couldn\\'t because of resource constraints. If you\\'re looking for a paper idea, feel free to continue where I left off.\\n\\n# Summary / Abstract\\n\\nThis study uncovers the factor of general intelligence, or *g*, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets—Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models—we find compelling evidence for a unidimensional, highly stable *g* factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and *g*. The discovery of the general intelligence factor in language models offers a unified metric for model evaluation and opens new avenues for more robust, *g*\\\\-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.\\n\\n# Arxiv enjoyers, I have a small request\\n\\nI want to put a preprint up on [cs.AI Arxiv](https://arxiv.org/list/cs.AI/recent) before I begin the publication process, but Arxiv is asking for endorsements. I don\\'t have anyone to ask, so I\\'m posting here.\\n\\nQuick edit: someone just endorsed it. Thank you whoever you are.\\n\\nArxiv link: [https://arxiv.org/abs/2310.11616](https://arxiv.org/abs/2310.11616) (also see paper below)\\n\\nEdit: I\\'ve been notified by multiple people that this paper is related to mine but I missed it and didn\\'t cite it. I\\'ll add it to my paper and contrast results after I read it, but here is it for the curious reader: [https://arxiv.org/abs/2306.10062](https://arxiv.org/abs/2306.10062)',\n",
       " 'Paper: [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)\\n\\nBlog: [https://palm-e.github.io/](https://palm-e.github.io/)\\n\\nTwitter: [https://twitter.com/DannyDriess/status/1632904675124035585](https://twitter.com/DannyDriess/status/1632904675124035585)\\n\\nAbstract:\\n\\n>Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, **exhibits positive transfer**: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. **Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.**       \\n\\nhttps://preview.redd.it/1z3zc3kte9ma1.jpg?width=1321&format=pjpg&auto=webp&s=bc191421a1e8db2faa50fb484073fd42d6d78a6a\\n\\nhttps://preview.redd.it/2qapt8kte9ma1.jpg?width=1180&format=pjpg&auto=webp&s=b1b75952a4f1e0e25036d606c4cc00366e983867\\n\\nhttps://preview.redd.it/thtfg6kte9ma1.jpg?width=725&format=pjpg&auto=webp&s=d6a2bebd723be8bcfeb787d3789169aa5aed60b3\\n\\nhttps://preview.redd.it/nffus6kte9ma1.jpg?width=712&format=pjpg&auto=webp&s=9f37d8e2e51b8b8ff53baa3d501b9ae4fe1119b8\\n\\nhttps://preview.redd.it/henjo3kte9ma1.jpg?width=710&format=pjpg&auto=webp&s=09f6d60cb4f814ba0395c2c1c27dfb86c4d56b00',\n",
       " 'A new benchmark designed to evaluate LLMs on real-world software engineering tasks pulls directly from Upwork freelance jobs with actual dollar values attached. The methodology involves collecting 1,400+ tasks ranging from $50-$32,000 in payout, creating standardized evaluation environments, and testing both coding ability and engineering management decisions.\\n\\nKey technical points:\\n- Tasks are verified through unit tests, expert validation, and comparison with human solutions\\n- Evaluation uses Docker containers to ensure consistent testing environments\\n- Includes both direct coding tasks and higher-level engineering management decisions\\n- Tasks span web development, mobile apps, data processing, and system architecture\\n- Total task value exceeds $1 million in real freelance payments\\n\\nI think this benchmark represents an important shift in how we evaluate LLMs for real-world applications. By tying performance directly to economic value, we can better understand the gap between current capabilities and practical utility. The low success rates suggest we need significant advances before LLMs can reliably handle professional software engineering tasks.\\n\\nI think the inclusion of management-level decisions is particularly valuable, as it tests both technical understanding and strategic thinking. This could help guide development of more complete engineering assistance systems.\\n\\nTLDR: New benchmark tests LLMs on real $1M+ worth of Upwork programming tasks. Current models struggle significantly, completing only ~10% of coding tasks and ~20% of management decisions.\\n\\n[Full summary is here](https://aimodels.fyi/papers/arxiv/swe-lancer-can-frontier-llms-earn-dollar1). Paper [here](https://arxiv.org/abs/2502.12115).',\n",
       " 'When visualizing the inner workings of vision transformers (ViTs), researchers noticed weird spikes of attention on random background patches. This didn\\'t make sense since the models should focus on foreground objects.\\n\\nBy analyzing the output embeddings, they found a small number of tokens (2%) had super high vector norms, causing the spikes.\\n\\nThe high-norm \"outlier\" tokens occurred in redundant areas and held less local info but more global info about the image.\\n\\nTheir hypothesis is that ViTs learn to identify unimportant patches and recycle them as temporary storage instead of discarding. This enables efficient processing but causes issues.\\n\\nTheir fix is simple - just add dedicated \"register\" tokens that provide storage space, avoiding the recycling side effects.\\n\\nModels trained with registers have:\\n\\n* Smoother and more meaningful attention maps\\n* Small boosts in downstream performance\\n* Way better object discovery abilities\\n\\nThe registers give ViTs a place to do their temporary computations without messing stuff up. Just a tiny architecture tweak improves interpretability and performance. Sweet!\\n\\nI think it\\'s cool how they reverse-engineered this model artifact and fixed it with such a small change. More work like this will keep incrementally improving ViTs.\\n\\nTLDR: Vision transformers recycle useless patches to store data, causing problems. Adding dedicated register tokens for storage fixes it nicely.\\n\\n[**Full summary**](https://notes.aimodels.fyi/demystifying-the-artifacts-in-vision-transformer-models/)**.** Paper is [here](https://arxiv.org/pdf/2309.16588.pdf).',\n",
       " 'A new study has uncovered that a significant fraction of peer reviews for top AI conferences in 2023-2024 likely included substantial AI-generated content from models like ChatGPT.\\n\\nUsing a novel statistical technique, researchers estimated the percentage of text generated by AI in large collections of documents. Analyzing peer reviews, they found:\\n\\n* 10.6% of ICLR 2024 reviews had significant AI content\\n* 9.1% for NeurIPS 2023\\n* 6.5% for CoRL 2023\\n* 16.9% for EMNLP 2023\\n\\nIn contrast, only 1-2% of pre-ChatGPT reviews from 2022 and earlier were flagged as having substantial AI contribution.\\n\\nSome key findings:\\n\\n1. AI-heavy reviews tended to come in close to the deadline\\n2. Fewer scholarly citations in AI-flavored reviews\\n3. Reviewers with AI-tinged reviews engaged less in author discussion\\n4. AI content made reviews more semantically homogeneous\\n5. Lower reviewer confidence correlated with higher AI estimates\\n\\nThe study, I think, raises some questions for proactive policy development in academia around responsible AI use in research. AI may be eroding the quality and integrity of peer review through these \"shadow\" influences. Open questions include:\\n\\n* Should AI assistance in peer review be disclosed?\\n* How should we incentivize good practices despite AI temptations?\\n* Can we preserve intellectual diversity under AI homogenization?\\n* Should we rethink credit for hybrid human/AI knowledge work?\\n\\nOverall, an interesting empirical glimpse into AI\\'s rapidly growing tendrils in the foundations of scientific quality control! I thought the approach of measuring the frequency of certain AI wording \"ticks\" made a lot of sense (some of the adjectives GPT4 uses, for example, are clear tells). \\n\\nI\\'m curious to read the comments on this one! I have a [much more detailed summary available here](https://aimodels.substack.com/p/new-study-finds-up-to-17-of-ai-conference) as well if you\\'re interested, and the original paper is [here](https://arxiv.org/pdf/2403.07183.pdf).',\n",
       " 'Ever since \\\\[Are emergent LLM abilities a mirage?\\\\]([https://arxiv.org/pdf/2304.15004.pdf](https://arxiv.org/pdf/2304.15004.pdf)), it seems like people have been awfully quiet about emergence. But the big \\\\[emergent abilities\\\\]([https://openreview.net/pdf?id=yzkSU5zdwD](https://openreview.net/pdf?id=yzkSU5zdwD)) paper has this paragraph (page 7):\\n\\n\\\\>  It is also important to consider the evaluation metrics used to measure emergent abilities (BIG-Bench, 2022). For instance, using exact string match as the evaluation metric for long-sequence targets may disguise compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic reasoning problems, where models are only scored on whether they get the final answer to a multi-step problem correct, without any credit given to partially correct solutions. However, the jump in final answer accuracy does not explain why the quality of intermediate steps suddenly emerges to above random, and using evaluation metrics that do not give partial credit are at best an incomplete explanation, because emergent abilities are still observed on many classification tasks (e.g., the tasks in Figure 2D–H).\\n\\nWhat do people think? Is emergence \"real\" or substantive?',\n",
       " \"Blog Post: https://sakana.ai/ai-scientist/\\n\\nPaper: https://arxiv.org/abs/2408.06292\\n\\nOpen-Source Project: https://github.com/SakanaAI/AI-Scientist\\n\\n**Abstract**\\n\\nOne of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aids to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems.\",\n",
       " '[PDF on ResearchGate](https://www.researchgate.net/publication/351476107_The_Modern_Mathematics_of_Deep_Learning) / [arXiv](https://arxiv.org/abs/2105.04026) (This review paper appears as a book chapter in the book [\"Mathematical Aspects of Deep Learning\"](https://doi.org/10.1017/9781009025096) by Cambridge University Press)\\n\\n**Abstract:**  We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.',\n",
       " 'Hey friends! I\\'m sharing this here because I think it warrants some attention, and I\\'m using methods that intersect from different domains, with Machine Learning being one of them.\\n\\nRecently I read Tegmark & co.\\'s paper on Geometric Concepts [https://arxiv.org/abs/2410.19750](https://arxiv.org/abs/2410.19750) and thought that it was fascinating that they were finding these geometric relationships in llms and wanted to tinker with their process a little bit, but I didn\\'t really have access or expertise to delve into LLM innards, so I thought I might be able to find something by mapping its output responses with embedding models to see if I can locate any geometric unity underlying how llms organize their semantic patterns. Well I did find that and more...\\n\\nI\\'ve made what I believe is a significant discovery about how meaning organizes itself geometrically in semantic space, and I\\'d like to share it with you and invite collaboration.\\n\\n**The Initial Discovery**\\n\\nWhile experimenting with different dimensionality reduction techniques (PCA, UMAP, t-SNE, and Isomap) to visualize semantic embeddings, I noticed something beautiful and striking; a consistent \"flower-like\" pattern emerging across all methods and combinations thereof. I systematically weeded out the possibility that this was the behavior of any single model(either embedding or dimensional reduction model) or combination of models and what I\\'ve found is kind of wild to say the least. It turns out that this wasn\\'t just a visualization artifact, as it appeared regardless of:\\n\\n\\\\- The reduction method used\\n\\n\\\\- The embedding model employed\\n\\n\\\\- The input text analyzed\\n\\nhttps://preview.redd.it/pdyq50s1ob2e1.png?width=907&format=png&auto=webp&s=b9ecf9206c1c2b43881341e8ad51950cf73b345c\\n\\nhttps://preview.redd.it/b2u3uz93ob2e1.png?width=1909&format=png&auto=webp&s=6448776ebaeb5620b2079c7fed6992b3a813d619\\n\\n[cross-section of the convergence point\\\\(Organic\\\\) hulls](https://preview.redd.it/t59tzz2qob2e1.png?width=1339&format=png&auto=webp&s=a9a0cd3132191db5a2ea163c87e8dfe336f9320c)\\n\\n[a step further, showing how they form with self similarity.](https://preview.redd.it/q0pmaveqob2e1.png?width=1339&format=png&auto=webp&s=863dd23a1899efc8bf266c0702cf3258643859c3)\\n\\n**Verification Through Multiple Methods**\\n\\nTo verify this isn\\'t just coincidental, I conducted several analyses, rewrote the program and math 4 times and did the following:\\n\\n1. Pairwise Similarity Matrices\\n\\nMapping the embeddings to similarity matrices reveals consistent patterns:\\n\\n\\\\- A perfect diagonal line (self-similarity = 1.0)\\n\\n\\\\- Regular cross-patterns at 45° angles\\n\\n\\\\- Repeating geometric structures\\n\\nhttps://preview.redd.it/ft89ukpaob2e1.png?width=460&format=png&auto=webp&s=9900f9113fad02841e5e18cb0bc5f9b6b66275e1\\n\\nhttps://preview.redd.it/f2yzbvnbob2e1.png?width=433&format=png&auto=webp&s=4a13a8e910794c64375ab0628f6f34006c31fb2f\\n\\nRelevant Code:  \\npython\\n\\ndef analyze\\\\_similarity\\\\_structure(embeddings):\\n\\nsimilarity\\\\_matrix = cosine\\\\_similarity(embeddings)\\n\\neigenvalues = np.linalg.eigvals(similarity\\\\_matrix)\\n\\nsorted\\\\_eigenvalues = sorted(eigenvalues, reverse=True)\\n\\nreturn similarity\\\\_matrix, sorted\\\\_eigenvalues\\n\\n2. Eigenvalue Analysis\\n\\nThe eigenvalue progression as more text is added, regardless of content or languages shows remarkable consistency like the following sample:\\n\\nFirst Set of eigenvalues while analyzing The Red Book by C.G. Jung in pieces:  \\n\\\\[35.39, 7.84, 6.71\\\\]\\n\\nLater Sets:  \\n\\\\[442.29, 162.38, 82.82\\\\]\\n\\n\\\\[533.16, 168.78, 95.53\\\\]\\n\\n\\\\[593.31, 172.75, 104.20\\\\]\\n\\n\\\\[619.62, 175.65, 109.41\\\\]\\n\\nhttps://preview.redd.it/hesf440job2e1.png?width=1088&format=png&auto=webp&s=b531499fe8043e0b41390229bd0b04017373c49b\\n\\nKey findings:\\n\\n\\\\- The top 3 eigenvalues consistently account for most of the variance\\n\\n\\\\- Clear logarithmic growth pattern\\n\\n\\\\- Stable spectral gaps i.e: (35.79393)\\n\\n3. Organic Hull Visualization\\n\\nThe geometric structure becomes particularly visible when visualizing through organic hulls:\\n\\nCode for generating data visualization through sinusoidal sphere deformations:  \\npython\\n\\ndef generate\\\\_organic\\\\_hull(points, method=\\'pca\\'):\\n\\nphi = np.linspace(0, 2\\\\*np.pi, 30)\\n\\ntheta = np.linspace(-np.pi/2, np.pi/2, 30)\\n\\nphi, theta = np.meshgrid(phi, theta)\\n\\ncenter = np.mean(points, axis=0)\\n\\nspread = np.std(points, axis=0)\\n\\nx = center\\\\[0\\\\] + spread\\\\[0\\\\] \\\\* np.cos(theta) \\\\* np.cos(phi)\\n\\ny = center\\\\[1\\\\] + spread\\\\[1\\\\] \\\\* np.cos(theta) \\\\* np.sin(phi)\\n\\nz = center\\\\[2\\\\] + spread\\\\[2\\\\] \\\\* np.sin(theta)\\n\\nreturn x, y, z\\n\\n\\\\`\\\\`\\\\`\\n\\nWhat the this discovery suggests is that meaning in semantic space has inherent geometric structure that organizes itself along predictable patterns and shows consistent mathematical self-similar relationships that exhibit golden ratio behavior like a penrose tiling, hyperbolic coxeter honeycomb etc and these patterns persist across combinations of different models and methods. I\\'ve run into an inverse of the problem that you have when you want to discover something; instead of finding a needle in a haystack, I\\'m trying to find a single piece of hay in a stack of needles, in the sense that nothing I do prevents these geometric unity from being present in the semantic space of all texts. The more text I throw at it, the more defined the geometry becomes.\\n\\nhttps://preview.redd.it/3hho1avzob2e1.png?width=1239&format=png&auto=webp&s=a446d6b71ba0166c842e9537c6cd228662bb2682\\n\\nI think I\\'ve done what I can so far on my own as far as cross-referencing results across multiple methods and collecting significant raw data that reinforces itself with each attempt to disprove it.\\n\\nSo I\\'m making a call for collaboration:\\n\\nI\\'m looking for collaborators interested in:\\n\\n1. Independently verifying these patterns\\n2. Exploring the mathematical implications\\n3. Investigating potential applications\\n4. Understanding the theoretical foundations\\n\\nMy complete codebase is available upon request, including:\\n\\n\\\\- Visualization tools\\n\\n\\\\- Analysis methods\\n\\n\\\\- Data processing pipeline\\n\\n\\\\- Metrics collection\\n\\nIf you\\'re interested in collaborating or would like to verify these findings independently, please reach out. This could have significant implications for our understanding of how meaning organizes itself and potentially for improving language models, cognitive science, data science and more.\\n\\n\\\\*TL;DR: Discovered consistent geometric patterns in semantic space across multiple reduction methods and embedding models, verified through similarity matrices and eigenvalue analysis. Looking for interested collaborators to explore this further and/or independently verify.\\n\\n\\\\##EDIT##: I\\n\\nI need to add some more context I guess,  because it seems that I\\'m being painted as a quack or a liar without being given the benefit of the doubt. Such is the nature of social media though I guess.\\n\\nThis is a cross-method, cross-model discovery using semantic embeddings that retain human interpretable relationships. i.e. for the similarity matrix visualizations, you can map the sentences to the eigenvalues and read them yourself. Theres nothing spooky going on here, its plain for your eyes and brain to see.\\n\\nHere are some other researchers who are like-minded and do it for a living.\\n\\n(Athanasopoulou et al.) supports our findings:\\n\\n\"The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it is organized in such a way that interesting semantic relations can be exported from manifolds of much lower dimensionality embedded in this high dimensional space.\" [https://aclanthology.org/C14-1069.pdf](https://aclanthology.org/C14-1069.pdf)\\n\\nA neuroscience paper(Alexander G. Huth 2013) reinforces my findings about geometric organization:\"An efficient way for the brain to represent object and action categories would be to organize them into a continuous space that reflects the semantic similarity between categories.\"  \\n[https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/)\\n\\n\"We use a novel eigenvector analysis method inspired from Random Matrix Theory and show that semantically coherent groups not only form in the row space, but also the column space.\"  \\n[https://openreview.net/pdf?id=rJfJiR5ooX](https://openreview.net/pdf?id=rJfJiR5ooX)\\n\\nI\\'m getting some hate here, but its unwarranted and comes from a lack of understanding. The automatic kneejerk reaction to completely shut someone down is not constructive criticism, its entirely unhelpful and unscientific in its closed-mindedness.',\n",
       " '**Paper:** [https://arxiv.org/pdf/2410.01131](https://arxiv.org/pdf/2410.01131)\\n\\n**Abstract:**\\n\\n>We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.\\n\\n**Highlights:**\\n\\n>Our key contributions are as follows:   \\n  \\n*Optimization of network parameters on the hypersphere* We propose to normalize all vectors forming the embedding dimensions of network matrices to lie on a unit norm hypersphere. This allows us to view matrix-vector multiplications as dot products representing cosine similarities bounded in \\\\[-1,1\\\\]. The normalization renders weight decay unnecessary.   \\n  \\n*Normalized Transformer as a variable-metric optimizer on the hypersphere* The normalized Transformer itself performs a multi-step optimization (two steps per layer) on a hypersphere, where each step of the attention and MLP updates is controlled by eigen learning rates—the diagonal elements of a learnable variable-metric matrix. For each token t\\\\_i in the input sequence, the optimization path of the normalized Transformer begins at a point on the hypersphere corresponding to its input embedding vector and moves to a point on the hypersphere that best predicts the embedding vector of the next token t\\\\_i+1 .   \\n  \\n*Faster convergence* We demonstrate that the normalized Transformer reduces the number of training steps required to achieve the same accuracy by a factor of 4 to 20.\\n\\n**Visual Highlights:**\\n\\nhttps://preview.redd.it/0jdj23ew6ytd1.png?width=1313&format=png&auto=webp&s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df\\n\\n[Not sure about the difference between 20k and 200k budgets; probably the best result from runs with different initial learning rates is plotted](https://preview.redd.it/8tf5tw0x6ytd1.png?width=1187&format=png&auto=webp&s=4f9dfbe1f49bdc8aed6fa953dc9220556d7dc947)\\n\\nhttps://preview.redd.it/waof2llr7ytd1.png?width=1337&format=png&auto=webp&s=3f82cee29c5fe753e219edf55ab16460fcf9a11a\\n\\nhttps://preview.redd.it/a5vburms7ytd1.png?width=859&format=png&auto=webp&s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691\\n\\n',\n",
       " 'Google has some serious cultural problems with proper credit assignment. They continue to rename methods discovered earlier DESPITE admitting the existence of this work.\\n\\nSee this new paper they released:\\n\\n[https://arxiv.org/abs/2006.14536](https://arxiv.org/abs/2006.14536)\\n\\nStop calling this method SWISH; its original name is SILU. The original Swish authors from Google even admitted to this mistake in the past ([https://www.reddit.com/r/MachineLearning/comments/773epu/r\\\\_swish\\\\_a\\\\_selfgated\\\\_activation\\\\_function\\\\_google/](https://www.reddit.com/r/MachineLearning/comments/773epu/r_swish_a_selfgated_activation_function_google/)). And the worst part is this new paper has the very same senior author as the previous Google paper.\\n\\nAnd just a couple weeks ago, the same issue again with the SimCLR paper. See thread here:\\n\\n[https://www.reddit.com/r/MachineLearning/comments/hbzd5o/d\\\\_on\\\\_the\\\\_public\\\\_advertising\\\\_of\\\\_neurips/fvcet9j/?utm\\\\_source=share&utm\\\\_medium=web2x](https://www.reddit.com/r/MachineLearning/comments/hbzd5o/d_on_the_public_advertising_of_neurips/fvcet9j/?utm_source=share&utm_medium=web2x)\\n\\nThey site only cite prior work with the same idea in the last paragraph of their supplementary and yet again rename the method to remove its association to the prior work. This is unfair. Unfair to the community and especially unfair to the lesser known researchers who do not have the advertising power of Geoff Hinton and Quoc Le on their papers.\\n\\nSiLU/Swish is by Stefan Elfwing, Eiji Uchibe, Kenji Doya ([https://arxiv.org/abs/1702.03118](https://arxiv.org/abs/1702.03118)).\\n\\nOriginal work of SimCLR is by Mang Ye, Xu Zhang, Pong C. Yuen, Shih-Fu Chang ([https://arxiv.org/abs/1904.03436](https://arxiv.org/abs/1904.03436))\\n\\nUpdate:\\n\\nDan Hendrycks and Kevin Gimpel also proposed the SiLU non-linearity in 2016 in their work Gaussian Error Linear Units (GELUs) ([https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415))\\n\\nUpdate 2:\\n\\n\"Smooth Adversarial Training\" by Cihang Xie is only an example of the renaming issue because of issues in the past by Google to properly assign credit. Cihang Xie\\'s work is not the cause of this issue. Their paper does not claim to discover a new activation function. They are only using the SiLU activation function in some of their experiments under the name Swish. [Cihang Xie will provide an update of the activation function naming used in the paper](https://www.reddit.com/r/MachineLearning/comments/hkiyir/r\\\\_google\\\\_has\\\\_a\\\\_credit\\\\_assignment\\\\_problem\\\\_in/fwtttqo?utm\\\\_source=share&utm\\\\_medium=web2x) to reflect the correct naming. \\n\\nThe cause of the issue is Google in the past decided to continue with renaming the activation as [Swish despite being made aware of the method already having the name SiLU](https://arxiv.org/abs/1710.05941). Now it is stuck in our research community and stuck in our ML libraries (https://github.com/tensorflow/tensorflow/issues/41066).',\n",
       " 'Recent research is shedding light on an unexpected problem in modern large language models, the deeper layers aren’t pulling their weight.\\n\\nA recent paper,\\xa0*\"The Curse of Depth in Large Language Models\"*, highlights a critical issue:  \\n\\\\- Deep layers in LLMs contribute significantly less to learning than earlier ones.  \\n\\\\- Many of these layers can be pruned without serious performance loss, raising questions about training efficiency.  \\n\\\\- The culprit? Pre-Layer Normalization (Pre-LN), which causes output variance to explode in deeper layers, making them act almost like identity functions.  \\n\\\\- A simple fix? LayerNorm Scaling, which controls this variance and improves training efficiency.\\n\\nThis has major implications for LLM architecture, training efficiency, and scaling laws. If half the layers in models like LLaMA, Mistral, and DeepSeek aren’t contributing effectively, how much computational waste are we dealing with?\\n\\nKey questions for discussion:  \\n1️) Should we be rethinking deep-layer training strategies to improve efficiency?  \\n2️) Does this impact the assumption that deeper = better in transformer architectures?  \\n3️) Could insights from this paper help with LLM compression, fine-tuning, or distillation techniques?\\n\\nPaper link:\\xa0[arXiv preprint: 2502.05795v1](https://arxiv.org/abs/2502.05795)\\n\\nLet’s discuss—what are your thoughts on the Curse of Depth?',\n",
       " \">Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the naïve loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and ⊥Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods.\\n\\nPaper: [https://arxiv.org/abs/2501.04697](https://arxiv.org/abs/2501.04697)\\n\\n(not my paper, just something that was recommended to me)  \\n\",\n",
       " 'I show that diffusion kernels capture global dependencies and that a simple diffusion kernel with a recurrent structure outperforms transformers in fewer parameters and FLOPs. \\n\\nhttps://arxiv.org/abs/2502.12381',\n",
       " 'DeepMind just published a paper about fact-checking text:\\n\\nhttps://preview.redd.it/zsmv0a0293rc1.png?width=1028&format=png&auto=webp&s=789c1c2f9b31aa734a7ebcf459df3ad06bd74285\\n\\nThe approach costs $0.19 per model response, using GPT-3.5-Turbo, which is cheaper than human annotators, while being more accurate than them:\\n\\nhttps://preview.redd.it/ob7bb3iv73rc1.png?width=1014&format=png&auto=webp&s=e79bbcaa578b29772cb3b43ead508daff7288091\\n\\nThey use this approach to create a factuality benchmark and compare some popular LLMs.\\n\\nPaper and code: [https://arxiv.org/abs/2403.18802](https://arxiv.org/abs/2403.18802)\\n\\nEDIT: Regarding the title of the post: Hallucination is defined (in Wikipedia) as \"a response generated by AI which contains false or\\xa0misleading information\\xa0presented as **fact**.\": Your code that does not compile is not, by itself, a hallucination. When you claim that the code is perfect, that\\'s a hallucination. ',\n",
       " 'Updated Paper [https://arxiv.org/pdf/2410.02162](https://arxiv.org/pdf/2410.02162) (includes results when paired w/ a verifier)\\n\\nOriginal Paper: [https://www.arxiv.org/abs/2409.13373](https://www.arxiv.org/abs/2409.13373)\\n\\n\"while o1’s performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it..\"\\n\\nThe summary is apt.  o1 looks to be a **very** impressive improvement.  At the same time, it reveals the remaining gaps:  degradation with increasing composition length,  100x cost, and huge degradation when \"retrieval\" is hampered via obfuscation of names.\\n\\nBut, I wonder if this is close enough. e.g. this type of model is at least sufficient to provide synthetic data / supervision to train a model that can fill these gaps.   If so, it won\\'t take long to find out, IMHO.\\n\\nAlso the authors have some spicy footnotes.  e.g. :\\n\\n\"The rich irony of researchers using tax payer provided research funds to pay private companies like OpenAI to evaluate their private commercial models is certainly not lost on us.\"',\n",
       " 'A research team from Google shows that replacing transformers’ self-attention sublayers with Fourier Transform achieves 92 percent of BERT accuracy on the GLUE benchmark with training times seven times faster on GPUs and twice as fast on TPUs.\\n\\nHere is a quick read: [Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs.](https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/)\\n\\nThe paper *FNet: Mixing Tokens with Fourier Transforms* is on [arXiv](https://arxiv.org/abs/2105.03824).']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.get_posts_sub('MachineLearning', 100, 'arxiv','day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'arxiv' in '[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:\\n\\n\"Given the breadth and depth of GPT-4\\'s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"\\n\\nWhat are everyone\\'s thoughts?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdf_getter as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pdf_getter' has no attribute 'find_pdf_link'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3161527/2581250972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_pdf_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://pubs.acs.org/doi/10.1021/acs.nanolett.4c05426'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pdf_getter' has no attribute 'find_pdf_link'"
     ]
    }
   ],
   "source": [
    "pg.find_pdf_link('https://pubs.acs.org/doi/10.1021/acs.nanolett.4c05426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
